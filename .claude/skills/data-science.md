# Data Science & Jupyter Workflows Skill

> ç»¼åˆè‡ªå¤šä¸ªå¼€æºä»“åº“å’Œæœ€ä½³å®è·µï¼Œå·²é€‚é… TAD æ¡†æ¶

## è§¦å‘æ¡ä»¶

å½“ç”¨æˆ·éœ€è¦è¿›è¡Œæ•°æ®åˆ†æã€æœºå™¨å­¦ä¹ å»ºæ¨¡ã€Jupyter Notebook å¼€å‘ã€æˆ–æ•°æ®ç§‘å­¦å·¥ä½œæµæ—¶ï¼Œè‡ªåŠ¨åº”ç”¨æ­¤ Skillã€‚

---

## æ ¸å¿ƒèƒ½åŠ›

```
æ•°æ®ç§‘å­¦å·¥å…·ç®±
â”œâ”€â”€ æ¢ç´¢æ€§åˆ†æ (EDA)
â”‚   â”œâ”€â”€ æ•°æ®æ¦‚è§ˆ
â”‚   â”œâ”€â”€ åˆ†å¸ƒåˆ†æ
â”‚   â””â”€â”€ ç›¸å…³æ€§åˆ†æ
â”œâ”€â”€ æ•°æ®é¢„å¤„ç†
â”‚   â”œâ”€â”€ ç¼ºå¤±å€¼å¤„ç†
â”‚   â”œâ”€â”€ ç‰¹å¾å·¥ç¨‹
â”‚   â””â”€â”€ æ•°æ®è½¬æ¢
â”œâ”€â”€ æœºå™¨å­¦ä¹ 
â”‚   â”œâ”€â”€ æ¨¡å‹é€‰æ‹©
â”‚   â”œâ”€â”€ è®­ç»ƒè¯„ä¼°
â”‚   â””â”€â”€ è¶…å‚è°ƒä¼˜
â”œâ”€â”€ å¯è§†åŒ–
â”‚   â”œâ”€â”€ ç»Ÿè®¡å›¾è¡¨
â”‚   â”œâ”€â”€ äº¤äº’å¼å›¾è¡¨
â”‚   â””â”€â”€ æ¨¡å‹è§£é‡Š
â””â”€â”€ Jupyter å·¥ä½œæµ
    â”œâ”€â”€ Notebook ç»“æ„
    â”œâ”€â”€ ä»£ç é‡æ„
    â””â”€â”€ ç”Ÿäº§åŒ–éƒ¨ç½²
```

---

## Jupyter Notebook æ ‡å‡†ç»“æ„

```python
# 1. ç¯å¢ƒè®¾ç½®
"""
# é¡¹ç›®åç§°
**ç›®æ ‡**: [æ˜ç¡®çš„åˆ†æç›®æ ‡]
**æ•°æ®**: [æ•°æ®æ¥æºå’Œæè¿°]
**ä½œè€…**: [å§“å]
**æ—¥æœŸ**: [åˆ›å»ºæ—¥æœŸ]
"""

# 2. å¯¼å…¥åº“
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

# è®¾ç½®æ˜¾ç¤ºé€‰é¡¹
pd.set_option('display.max_columns', None)
plt.style.use('seaborn-v0_8-whitegrid')
%matplotlib inline

# 3. æ•°æ®åŠ è½½
# 4. æ¢ç´¢æ€§åˆ†æ (EDA)
# 5. æ•°æ®é¢„å¤„ç†
# 6. ç‰¹å¾å·¥ç¨‹
# 7. æ¨¡å‹è®­ç»ƒ
# 8. æ¨¡å‹è¯„ä¼°
# 9. ç»“è®ºä¸å»ºè®®
```

---

## æ¢ç´¢æ€§æ•°æ®åˆ†æ (EDA)

### å¿«é€Ÿæ•°æ®æ¦‚è§ˆ

```python
def data_overview(df):
    """ç”Ÿæˆæ•°æ®é›†çš„å®Œæ•´æ¦‚è§ˆ"""
    print("=" * 50)
    print("æ•°æ®é›†æ¦‚è§ˆ")
    print("=" * 50)

    print(f"\nğŸ“Š æ•°æ®è§„æ¨¡: {df.shape[0]:,} è¡Œ Ã— {df.shape[1]} åˆ—")

    print(f"\nğŸ“‹ æ•°æ®ç±»å‹åˆ†å¸ƒ:")
    print(df.dtypes.value_counts())

    print(f"\nâ“ ç¼ºå¤±å€¼ç»Ÿè®¡:")
    missing = df.isnull().sum()
    missing_pct = (missing / len(df) * 100).round(2)
    missing_df = pd.DataFrame({
        'ç¼ºå¤±æ•°é‡': missing,
        'ç¼ºå¤±æ¯”ä¾‹%': missing_pct
    })
    print(missing_df[missing_df['ç¼ºå¤±æ•°é‡'] > 0])

    print(f"\nğŸ”¢ æ•°å€¼åˆ—ç»Ÿè®¡:")
    print(df.describe().round(2))

    print(f"\nğŸ“ åˆ†ç±»åˆ—ç»Ÿè®¡:")
    for col in df.select_dtypes(include='object').columns:
        print(f"\n{col}: {df[col].nunique()} ä¸ªå”¯ä¸€å€¼")
        print(df[col].value_counts().head())

# ä½¿ç”¨
data_overview(df)
```

### å¯è§†åŒ–åˆ†æ

```python
def plot_distributions(df, figsize=(15, 10)):
    """ç»˜åˆ¶æ‰€æœ‰æ•°å€¼åˆ—çš„åˆ†å¸ƒå›¾"""
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    n_cols = 3
    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols

    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)
    axes = axes.flatten()

    for i, col in enumerate(numeric_cols):
        axes[i].hist(df[col].dropna(), bins=30, edgecolor='black', alpha=0.7)
        axes[i].set_title(col)
        axes[i].set_xlabel('')

    # éšè—å¤šä½™çš„å­å›¾
    for j in range(i + 1, len(axes)):
        axes[j].set_visible(False)

    plt.tight_layout()
    plt.show()

def plot_correlation_matrix(df, figsize=(12, 10)):
    """ç»˜åˆ¶ç›¸å…³æ€§çƒ­åŠ›å›¾"""
    numeric_df = df.select_dtypes(include=[np.number])
    corr = numeric_df.corr()

    plt.figure(figsize=figsize)
    mask = np.triu(np.ones_like(corr, dtype=bool))
    sns.heatmap(corr, mask=mask, annot=True, fmt='.2f',
                cmap='RdBu_r', center=0, square=True)
    plt.title('ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ')
    plt.tight_layout()
    plt.show()
```

---

## æ•°æ®é¢„å¤„ç†

### ç¼ºå¤±å€¼å¤„ç†

```python
class MissingValueHandler:
    """ç¼ºå¤±å€¼å¤„ç†å·¥å…·"""

    @staticmethod
    def fill_numeric(df, strategy='median'):
        """å¡«å……æ•°å€¼åˆ—ç¼ºå¤±å€¼"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if df[col].isnull().sum() > 0:
                if strategy == 'median':
                    df[col].fillna(df[col].median(), inplace=True)
                elif strategy == 'mean':
                    df[col].fillna(df[col].mean(), inplace=True)
                elif strategy == 'zero':
                    df[col].fillna(0, inplace=True)
        return df

    @staticmethod
    def fill_categorical(df, strategy='mode'):
        """å¡«å……åˆ†ç±»åˆ—ç¼ºå¤±å€¼"""
        cat_cols = df.select_dtypes(include='object').columns
        for col in cat_cols:
            if df[col].isnull().sum() > 0:
                if strategy == 'mode':
                    df[col].fillna(df[col].mode()[0], inplace=True)
                elif strategy == 'unknown':
                    df[col].fillna('Unknown', inplace=True)
        return df

    @staticmethod
    def drop_high_missing(df, threshold=0.5):
        """åˆ é™¤ç¼ºå¤±ç‡è¶…è¿‡é˜ˆå€¼çš„åˆ—"""
        missing_pct = df.isnull().sum() / len(df)
        cols_to_drop = missing_pct[missing_pct > threshold].index
        print(f"åˆ é™¤åˆ—: {list(cols_to_drop)}")
        return df.drop(columns=cols_to_drop)
```

### ç‰¹å¾å·¥ç¨‹

```python
class FeatureEngineer:
    """ç‰¹å¾å·¥ç¨‹å·¥å…·"""

    @staticmethod
    def create_datetime_features(df, date_col):
        """ä»æ—¥æœŸåˆ—æå–ç‰¹å¾"""
        df[date_col] = pd.to_datetime(df[date_col])
        df[f'{date_col}_year'] = df[date_col].dt.year
        df[f'{date_col}_month'] = df[date_col].dt.month
        df[f'{date_col}_day'] = df[date_col].dt.day
        df[f'{date_col}_dayofweek'] = df[date_col].dt.dayofweek
        df[f'{date_col}_is_weekend'] = df[date_col].dt.dayofweek >= 5
        return df

    @staticmethod
    def create_binned_features(df, col, bins, labels=None):
        """åˆ›å»ºåˆ†ç®±ç‰¹å¾"""
        df[f'{col}_binned'] = pd.cut(df[col], bins=bins, labels=labels)
        return df

    @staticmethod
    def encode_categorical(df, cols, method='onehot'):
        """ç¼–ç åˆ†ç±»å˜é‡"""
        if method == 'onehot':
            return pd.get_dummies(df, columns=cols, drop_first=True)
        elif method == 'label':
            from sklearn.preprocessing import LabelEncoder
            le = LabelEncoder()
            for col in cols:
                df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))
            return df
```

---

## æœºå™¨å­¦ä¹ å·¥ä½œæµ

### æ¨¡å‹è®­ç»ƒæ¨¡æ¿

```python
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression

class MLPipeline:
    """æœºå™¨å­¦ä¹ æµæ°´çº¿"""

    def __init__(self, X, y, test_size=0.2, random_state=42):
        self.X_train, self.X_test, self.y_train, self.y_test = \
            train_test_split(X, y, test_size=test_size, random_state=random_state)

        # æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)

        self.models = {}
        self.results = {}

    def train_models(self):
        """è®­ç»ƒå¤šä¸ªæ¨¡å‹"""
        models = {
            'Logistic Regression': LogisticRegression(max_iter=1000),
            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
            'Gradient Boosting': GradientBoostingClassifier(random_state=42)
        }

        for name, model in models.items():
            print(f"\nè®­ç»ƒ {name}...")

            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=5)

            # è®­ç»ƒ
            model.fit(self.X_train_scaled, self.y_train)

            # é¢„æµ‹
            y_pred = model.predict(self.X_test_scaled)

            # è¯„ä¼°
            self.models[name] = model
            self.results[name] = {
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'test_accuracy': accuracy_score(self.y_test, y_pred)
            }

            print(f"  CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
            print(f"  Test Accuracy: {accuracy_score(self.y_test, y_pred):.4f}")

    def compare_models(self):
        """æ¯”è¾ƒæ¨¡å‹ç»“æœ"""
        results_df = pd.DataFrame(self.results).T
        results_df = results_df.sort_values('test_accuracy', ascending=False)
        return results_df

    def get_best_model(self):
        """è·å–æœ€ä½³æ¨¡å‹"""
        best_name = max(self.results, key=lambda k: self.results[k]['test_accuracy'])
        return best_name, self.models[best_name]
```

### æ¨¡å‹è§£é‡Š

```python
def plot_feature_importance(model, feature_names, top_n=20):
    """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
    if hasattr(model, 'feature_importances_'):
        importance = model.feature_importances_
    elif hasattr(model, 'coef_'):
        importance = np.abs(model.coef_[0])
    else:
        print("æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§")
        return

    # æ’åº
    indices = np.argsort(importance)[::-1][:top_n]

    plt.figure(figsize=(10, 8))
    plt.barh(range(len(indices)), importance[indices])
    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
    plt.xlabel('é‡è¦æ€§')
    plt.title(f'Top {top_n} ç‰¹å¾é‡è¦æ€§')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()
```

---

## Notebook æœ€ä½³å®è·µ

### ä»£ç ç»„ç»‡

```
âœ… æ¨è
â–¡ ä½¿ç”¨ Markdown å•å…ƒæ ¼è§£é‡Šæ¯ä¸ªæ­¥éª¤
â–¡ æ¯ä¸ªå•å…ƒæ ¼åªåšä¸€ä»¶äº‹
â–¡ å°†é‡å¤ä»£ç æå–ä¸ºå‡½æ•°
â–¡ ä½¿ç”¨æœ‰æ„ä¹‰çš„å˜é‡å
â–¡ åœ¨ Notebook å¼€å¤´åˆ—å‡ºæ‰€æœ‰ä¾èµ–

âŒ é¿å…
â–¡ è¶…é•¿çš„ä»£ç å•å…ƒæ ¼
â–¡ æœªæ³¨é‡Šçš„å¤æ‚é€»è¾‘
â–¡ ç¡¬ç¼–ç çš„è·¯å¾„å’Œå‚æ•°
â–¡ æœªå¤„ç†çš„è­¦å‘Šä¿¡æ¯
â–¡ è¿è¡Œé¡ºåºä¾èµ–ï¼ˆéœ€ä»å¤´è¿è¡Œæ‰èƒ½å·¥ä½œï¼‰
```

### ç‰ˆæœ¬æ§åˆ¶å‹å¥½

```python
# åœ¨ Notebook å¼€å¤´æ·»åŠ 
%load_ext autoreload
%autoreload 2

# å°†æ ¸å¿ƒåŠŸèƒ½ç§»åˆ° .py æ–‡ä»¶
# ä¾‹å¦‚: src/preprocessing.py, src/models.py
from src.preprocessing import clean_data
from src.models import train_model
```

### ç”Ÿäº§åŒ–è½¬æ¢

```python
# å°† Notebook è½¬æ¢ä¸ºè„šæœ¬
# jupyter nbconvert --to script notebook.ipynb

# æˆ–ä½¿ç”¨ nbdev æ¡†æ¶
# pip install nbdev
# nbdev_export
```

---

## å¸¸ç”¨å¯è§†åŒ–æ¨¡æ¿

### åˆ†ç±»é—®é¢˜å¯è§†åŒ–

```python
def plot_classification_results(y_true, y_pred, labels=None):
    """åˆ†ç±»ç»“æœå¯è§†åŒ–"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])
    axes[0].set_title('æ··æ·†çŸ©é˜µ')
    axes[0].set_xlabel('é¢„æµ‹å€¼')
    axes[0].set_ylabel('çœŸå®å€¼')

    # åˆ†ç±»æŠ¥å‘Š
    report = classification_report(y_true, y_pred, output_dict=True)
    report_df = pd.DataFrame(report).T.iloc[:-3, :-1]
    report_df.plot(kind='bar', ax=axes[1])
    axes[1].set_title('åˆ†ç±»æŠ¥å‘Š')
    axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45)
    axes[1].legend(loc='lower right')

    plt.tight_layout()
    plt.show()
```

### å›å½’é—®é¢˜å¯è§†åŒ–

```python
def plot_regression_results(y_true, y_pred):
    """å›å½’ç»“æœå¯è§†åŒ–"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # é¢„æµ‹ vs çœŸå®
    axes[0].scatter(y_true, y_pred, alpha=0.5)
    axes[0].plot([y_true.min(), y_true.max()],
                 [y_true.min(), y_true.max()], 'r--', lw=2)
    axes[0].set_xlabel('çœŸå®å€¼')
    axes[0].set_ylabel('é¢„æµ‹å€¼')
    axes[0].set_title('é¢„æµ‹ vs çœŸå®')

    # æ®‹å·®åˆ†å¸ƒ
    residuals = y_true - y_pred
    axes[1].hist(residuals, bins=30, edgecolor='black', alpha=0.7)
    axes[1].axvline(x=0, color='r', linestyle='--')
    axes[1].set_xlabel('æ®‹å·®')
    axes[1].set_ylabel('é¢‘æ•°')
    axes[1].set_title('æ®‹å·®åˆ†å¸ƒ')

    plt.tight_layout()
    plt.show()
```

---

## ä¸ TAD æ¡†æ¶çš„é›†æˆ

åœ¨ TAD çš„æ•°æ®åˆ†ææµç¨‹ä¸­ï¼š

```
ä¸šåŠ¡é—®é¢˜ â†’ æ•°æ®è·å– â†’ EDA â†’ ç‰¹å¾å·¥ç¨‹ â†’ å»ºæ¨¡ â†’ è¯„ä¼° â†’ éƒ¨ç½²
               â†“
          [ æ­¤ Skill ]
```

**ä½¿ç”¨åœºæ™¯**ï¼š
- æ¢ç´¢æ€§æ•°æ®åˆ†æ
- æœºå™¨å­¦ä¹ æ¨¡å‹å¼€å‘
- A/B æµ‹è¯•åˆ†æ
- é¢„æµ‹å»ºæ¨¡
- æ•°æ®æŠ¥å‘Šç”Ÿæˆ

---

*æ­¤ Skill å¸®åŠ© Claude è¿›è¡Œé«˜æ•ˆçš„æ•°æ®ç§‘å­¦å·¥ä½œæµã€‚*
