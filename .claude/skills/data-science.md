# Data Science & Jupyter Workflows Skill

---
title: "Data Science & Jupyter Workflows"
version: "3.0"
last_updated: "2026-01-06"
tags: [data-science, ml, jupyter, mlops, reproducibility, experiment-tracking]
domains: [data, ml, analytics]
level: intermediate
estimated_time: "60min"
prerequisites: [python]
sources:
  - "MLflow Documentation"
  - "Weights & Biases Best Practices"
  - "Google ML Best Practices"
  - "Model Cards for Model Reporting (Mitchell et al.)"
enforcement: recommended
tad_gates: [Gate2_Design, Gate3_Testing, Gate4_Review]
---

> ç»¼åˆè‡ªå¤šä¸ªå¼€æºä»“åº“ã€MLOps æœ€ä½³å®è·µå’Œè´Ÿè´£ä»» AI æŒ‡å—ï¼Œå·²é€‚é… TAD æ¡†æ¶

## TL;DR Quick Checklist

```
1. [ ] Set random seeds for reproducibility
2. [ ] Lock dependencies (requirements.txt / conda.yaml)
3. [ ] Track experiments with MLflow/W&B
4. [ ] Version datasets with DVC
5. [ ] Document model with Model Card
6. [ ] Validate with hold-out test set
```

**Red Flags:**
- No random seed set
- Missing dependency versions
- No experiment tracking
- Unreproducible results across runs
- No model documentation
- Training on test data (data leakage)

---

## è§¦å‘æ¡ä»¶

å½“ç”¨æˆ·éœ€è¦è¿›è¡Œæ•°æ®åˆ†æã€æœºå™¨å­¦ä¹ å»ºæ¨¡ã€Jupyter Notebook å¼€å‘ã€æˆ–æ•°æ®ç§‘å­¦å·¥ä½œæµæ—¶ï¼Œè‡ªåŠ¨åº”ç”¨æ­¤ Skillã€‚

---

## æ ¸å¿ƒèƒ½åŠ›

```
æ•°æ®ç§‘å­¦å·¥å…·ç®±
â”œâ”€â”€ æ¢ç´¢æ€§åˆ†æ (EDA)
â”‚   â”œâ”€â”€ æ•°æ®æ¦‚è§ˆ
â”‚   â”œâ”€â”€ åˆ†å¸ƒåˆ†æ
â”‚   â””â”€â”€ ç›¸å…³æ€§åˆ†æ
â”œâ”€â”€ æ•°æ®é¢„å¤„ç†
â”‚   â”œâ”€â”€ ç¼ºå¤±å€¼å¤„ç†
â”‚   â”œâ”€â”€ ç‰¹å¾å·¥ç¨‹
â”‚   â””â”€â”€ æ•°æ®è½¬æ¢
â”œâ”€â”€ æœºå™¨å­¦ä¹ 
â”‚   â”œâ”€â”€ æ¨¡å‹é€‰æ‹©
â”‚   â”œâ”€â”€ è®­ç»ƒè¯„ä¼°
â”‚   â””â”€â”€ è¶…å‚è°ƒä¼˜
â”œâ”€â”€ å¯è§†åŒ–
â”‚   â”œâ”€â”€ ç»Ÿè®¡å›¾è¡¨
â”‚   â”œâ”€â”€ äº¤äº’å¼å›¾è¡¨
â”‚   â””â”€â”€ æ¨¡å‹è§£é‡Š
â””â”€â”€ Jupyter å·¥ä½œæµ
    â”œâ”€â”€ Notebook ç»“æ„
    â”œâ”€â”€ ä»£ç é‡æ„
    â””â”€â”€ ç”Ÿäº§åŒ–éƒ¨ç½²
```

---

## Reproducibility (å¤ç°æ€§)

Reproducibility is **non-negotiable** for scientific validity and production reliability.

### Setting Random Seeds

```python
import os
import random
import numpy as np
import torch  # if using PyTorch

def set_seed(seed: int = 42):
    """Set all random seeds for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

    # PyTorch
    if 'torch' in dir():
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    # TensorFlow
    try:
        import tensorflow as tf
        tf.random.set_seed(seed)
    except ImportError:
        pass

# Call at the START of every script/notebook
SEED = 42
set_seed(SEED)

# Also pass to sklearn functions
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=SEED
)
```

### Environment Management

```yaml
# environment.yml (Conda - RECOMMENDED)
name: my-ml-project
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.11.5
  - numpy=1.26.2
  - pandas=2.1.3
  - scikit-learn=1.3.2
  - matplotlib=3.8.2
  - seaborn=0.13.0
  - jupyter=1.0.0
  - pip:
    - mlflow==2.9.2
    - wandb==0.16.1
```

```bash
# Create and export environment
conda env create -f environment.yml
conda activate my-ml-project
conda env export > environment.lock.yml  # Full lockfile

# Pip alternative
pip freeze > requirements.txt

# Or use pip-tools for better dependency resolution
pip-compile requirements.in -o requirements.txt
```

### Docker for Full Reproducibility

```dockerfile
# Dockerfile
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy and install dependencies first (better caching)
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy source code
COPY . .

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV PYTHONHASHSEED=42

# Default command
CMD ["python", "train.py"]
```

### Reproducibility Checklist

```markdown
## Reproducibility Verification

### Environment
- [ ] Python version pinned
- [ ] All package versions locked
- [ ] CUDA/cuDNN versions documented (if GPU)
- [ ] Docker image available (optional but recommended)

### Code
- [ ] Random seeds set at script start
- [ ] Seeds passed to all random functions
- [ ] Data loading is deterministic
- [ ] No hidden state between runs

### Data
- [ ] Data version tracked (hash or DVC)
- [ ] Train/val/test splits saved or reproducible
- [ ] Preprocessing steps documented
- [ ] No data leakage between splits

### Results
- [ ] Metrics match across repeated runs (within tolerance)
- [ ] Model weights can be reloaded
- [ ] Inference results are reproducible
```

---

## Experiment Tracking (å®éªŒè¿½è¸ª)

Track every experiment systematically to avoid "which model was best again?"

### MLflow Setup

```python
import mlflow
import mlflow.sklearn
from sklearn.metrics import accuracy_score, f1_score

# Set tracking URI (local or remote)
mlflow.set_tracking_uri("sqlite:///mlflow.db")  # Local
# mlflow.set_tracking_uri("http://mlflow-server:5000")  # Remote

# Set experiment name
mlflow.set_experiment("customer-churn-prediction")

# Training with tracking
def train_with_tracking(X_train, y_train, X_test, y_test, params):
    with mlflow.start_run(run_name=f"rf_{params['n_estimators']}trees"):
        # Log parameters
        mlflow.log_params(params)
        mlflow.log_param("dataset_version", "v2.1")

        # Train model
        model = RandomForestClassifier(**params, random_state=SEED)
        model.fit(X_train, y_train)

        # Evaluate
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')

        # Log metrics
        mlflow.log_metrics({
            "accuracy": accuracy,
            "f1_score": f1,
            "train_samples": len(X_train),
            "test_samples": len(X_test)
        })

        # Log model
        mlflow.sklearn.log_model(model, "model")

        # Log artifacts (plots, reports)
        # mlflow.log_artifact("confusion_matrix.png")

        # Log tags for organization
        mlflow.set_tags({
            "model_type": "random_forest",
            "feature_set": "v3",
            "author": "data-team"
        })

        return model, accuracy

# Run experiments
for n_trees in [50, 100, 200]:
    params = {"n_estimators": n_trees, "max_depth": 10}
    train_with_tracking(X_train, y_train, X_test, y_test, params)
```

### Weights & Biases (W&B)

```python
import wandb
from wandb.integration.sklearn import plot_confusion_matrix

# Initialize project
wandb.init(
    project="customer-churn",
    name="experiment-001",
    config={
        "model": "random_forest",
        "n_estimators": 100,
        "learning_rate": 0.01,
        "dataset": "churn_v2"
    }
)

# Training loop with logging
for epoch in range(epochs):
    # ... training code ...

    # Log metrics
    wandb.log({
        "epoch": epoch,
        "train_loss": train_loss,
        "val_loss": val_loss,
        "val_accuracy": val_accuracy
    })

# Log confusion matrix
wandb.sklearn.plot_confusion_matrix(y_test, y_pred, labels=class_names)

# Log model artifact
artifact = wandb.Artifact("model", type="model")
artifact.add_file("model.pkl")
wandb.log_artifact(artifact)

wandb.finish()
```

### DVC for Data Versioning

```bash
# Initialize DVC
dvc init

# Track data files
dvc add data/raw/customers.csv
git add data/raw/customers.csv.dvc .gitignore
git commit -m "Add raw customer data"

# Create data pipeline
dvc run -n preprocess \
    -d data/raw/customers.csv \
    -d src/preprocess.py \
    -o data/processed/customers_clean.csv \
    python src/preprocess.py

dvc run -n train \
    -d data/processed/customers_clean.csv \
    -d src/train.py \
    -o models/model.pkl \
    -M metrics.json \
    python src/train.py

# dvc.yaml is created automatically
# Push data to remote storage
dvc remote add -d storage s3://my-bucket/dvc
dvc push
```

### Experiment Comparison

```python
# MLflow: Compare runs programmatically
import mlflow
from mlflow.tracking import MlflowClient

client = MlflowClient()
experiment = client.get_experiment_by_name("customer-churn-prediction")

# Get all runs
runs = client.search_runs(
    experiment_ids=[experiment.experiment_id],
    order_by=["metrics.accuracy DESC"],
    max_results=10
)

# Compare
for run in runs:
    print(f"Run: {run.info.run_name}")
    print(f"  Accuracy: {run.data.metrics['accuracy']:.4f}")
    print(f"  Params: {run.data.params}")
```

---

## Model Cards (æ¨¡å‹å¡)

Model Cards document model capabilities, limitations, and intended use for responsible AI.

### Model Card Template

```markdown
# Model Card: Customer Churn Predictor

## Model Details

| Attribute | Value |
|-----------|-------|
| **Model Name** | ChurnPredictor-v2.1 |
| **Model Type** | Gradient Boosting Classifier |
| **Version** | 2.1.0 |
| **Release Date** | 2026-01-06 |
| **Developers** | Data Science Team |
| **License** | Internal Use Only |
| **Contact** | ml-team@company.com |

## Intended Use

### Primary Use Case
Predict customer churn probability to enable proactive retention campaigns.

### Intended Users
- Customer Success Team
- Marketing Automation Systems

### Out-of-Scope Uses
- âŒ Credit decisions
- âŒ Employment decisions
- âŒ Individual customer targeting without consent

## Training Data

| Attribute | Value |
|-----------|-------|
| **Dataset** | customer_data_v5 |
| **Size** | 150,000 records |
| **Time Period** | 2023-01 to 2025-12 |
| **Features** | 45 (demographics, behavior, transactions) |
| **Label** | Churned within 90 days (binary) |

### Data Distribution
- Churn Rate: 18%
- Geographic: 60% US, 25% EU, 15% APAC
- Customer Tenure: Mean 2.3 years

## Performance

### Overall Metrics
| Metric | Value |
|--------|-------|
| Accuracy | 0.87 |
| Precision | 0.82 |
| Recall | 0.78 |
| F1 Score | 0.80 |
| AUC-ROC | 0.91 |

### Performance by Subgroup
| Segment | Accuracy | Precision | Recall |
|---------|----------|-----------|--------|
| US Customers | 0.89 | 0.84 | 0.81 |
| EU Customers | 0.86 | 0.80 | 0.76 |
| APAC Customers | 0.83 | 0.78 | 0.72 |
| < 1 year tenure | 0.84 | 0.79 | 0.74 |
| > 3 year tenure | 0.90 | 0.86 | 0.83 |

## Limitations

### Known Limitations
- Lower performance for APAC region (less training data)
- May not generalize to B2B customers (trained on B2C)
- Seasonal patterns not fully captured

### Failure Modes
- Poor prediction for customers with < 30 days of data
- May overpredict churn during promotional periods

## Ethical Considerations

### Fairness
- Model tested for demographic parity
- No protected attributes used directly
- Regular bias audits scheduled

### Privacy
- No PII in features
- Aggregated behavioral data only
- GDPR compliant

## Maintenance

| Attribute | Value |
|-----------|-------|
| **Retraining Frequency** | Monthly |
| **Monitoring** | Daily drift detection |
| **Owner** | ML Platform Team |
| **Last Audit** | 2025-12-15 |
```

### Model Card in Code (Hugging Face Format)

```python
from huggingface_hub import ModelCard, ModelCardData

card_data = ModelCardData(
    language='en',
    license='mit',
    library_name='sklearn',
    tags=['classification', 'churn', 'tabular'],
    datasets=['company/customer-data-v5'],
    metrics=[
        {'type': 'accuracy', 'value': 0.87},
        {'type': 'f1', 'value': 0.80}
    ],
    model_name='ChurnPredictor-v2.1'
)

card = ModelCard.from_template(
    card_data,
    model_id="company/churn-predictor",
    model_description="Predicts customer churn probability",
    developers="Data Science Team",
    model_type="Gradient Boosting Classifier"
)

# Save to README.md
card.save("models/churn-predictor/README.md")
```

---

## Jupyter Notebook æ ‡å‡†ç»“æ„

```python
# 1. ç¯å¢ƒè®¾ç½®
"""
# é¡¹ç›®åç§°
**ç›®æ ‡**: [æ˜ç¡®çš„åˆ†æç›®æ ‡]
**æ•°æ®**: [æ•°æ®æ¥æºå’Œæè¿°]
**ä½œè€…**: [å§“å]
**æ—¥æœŸ**: [åˆ›å»ºæ—¥æœŸ]
"""

# 2. å¯¼å…¥åº“
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

# è®¾ç½®æ˜¾ç¤ºé€‰é¡¹
pd.set_option('display.max_columns', None)
plt.style.use('seaborn-v0_8-whitegrid')
%matplotlib inline

# 3. æ•°æ®åŠ è½½
# 4. æ¢ç´¢æ€§åˆ†æ (EDA)
# 5. æ•°æ®é¢„å¤„ç†
# 6. ç‰¹å¾å·¥ç¨‹
# 7. æ¨¡å‹è®­ç»ƒ
# 8. æ¨¡å‹è¯„ä¼°
# 9. ç»“è®ºä¸å»ºè®®
```

---

## æ¢ç´¢æ€§æ•°æ®åˆ†æ (EDA)

### å¿«é€Ÿæ•°æ®æ¦‚è§ˆ

```python
def data_overview(df):
    """ç”Ÿæˆæ•°æ®é›†çš„å®Œæ•´æ¦‚è§ˆ"""
    print("=" * 50)
    print("æ•°æ®é›†æ¦‚è§ˆ")
    print("=" * 50)

    print(f"\nğŸ“Š æ•°æ®è§„æ¨¡: {df.shape[0]:,} è¡Œ Ã— {df.shape[1]} åˆ—")

    print(f"\nğŸ“‹ æ•°æ®ç±»å‹åˆ†å¸ƒ:")
    print(df.dtypes.value_counts())

    print(f"\nâ“ ç¼ºå¤±å€¼ç»Ÿè®¡:")
    missing = df.isnull().sum()
    missing_pct = (missing / len(df) * 100).round(2)
    missing_df = pd.DataFrame({
        'ç¼ºå¤±æ•°é‡': missing,
        'ç¼ºå¤±æ¯”ä¾‹%': missing_pct
    })
    print(missing_df[missing_df['ç¼ºå¤±æ•°é‡'] > 0])

    print(f"\nğŸ”¢ æ•°å€¼åˆ—ç»Ÿè®¡:")
    print(df.describe().round(2))

    print(f"\nğŸ“ åˆ†ç±»åˆ—ç»Ÿè®¡:")
    for col in df.select_dtypes(include='object').columns:
        print(f"\n{col}: {df[col].nunique()} ä¸ªå”¯ä¸€å€¼")
        print(df[col].value_counts().head())

# ä½¿ç”¨
data_overview(df)
```

### å¯è§†åŒ–åˆ†æ

```python
def plot_distributions(df, figsize=(15, 10)):
    """ç»˜åˆ¶æ‰€æœ‰æ•°å€¼åˆ—çš„åˆ†å¸ƒå›¾"""
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    n_cols = 3
    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols

    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)
    axes = axes.flatten()

    for i, col in enumerate(numeric_cols):
        axes[i].hist(df[col].dropna(), bins=30, edgecolor='black', alpha=0.7)
        axes[i].set_title(col)
        axes[i].set_xlabel('')

    # éšè—å¤šä½™çš„å­å›¾
    for j in range(i + 1, len(axes)):
        axes[j].set_visible(False)

    plt.tight_layout()
    plt.show()

def plot_correlation_matrix(df, figsize=(12, 10)):
    """ç»˜åˆ¶ç›¸å…³æ€§çƒ­åŠ›å›¾"""
    numeric_df = df.select_dtypes(include=[np.number])
    corr = numeric_df.corr()

    plt.figure(figsize=figsize)
    mask = np.triu(np.ones_like(corr, dtype=bool))
    sns.heatmap(corr, mask=mask, annot=True, fmt='.2f',
                cmap='RdBu_r', center=0, square=True)
    plt.title('ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ')
    plt.tight_layout()
    plt.show()
```

---

## æ•°æ®é¢„å¤„ç†

### ç¼ºå¤±å€¼å¤„ç†

```python
class MissingValueHandler:
    """ç¼ºå¤±å€¼å¤„ç†å·¥å…·"""

    @staticmethod
    def fill_numeric(df, strategy='median'):
        """å¡«å……æ•°å€¼åˆ—ç¼ºå¤±å€¼"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if df[col].isnull().sum() > 0:
                if strategy == 'median':
                    df[col].fillna(df[col].median(), inplace=True)
                elif strategy == 'mean':
                    df[col].fillna(df[col].mean(), inplace=True)
                elif strategy == 'zero':
                    df[col].fillna(0, inplace=True)
        return df

    @staticmethod
    def fill_categorical(df, strategy='mode'):
        """å¡«å……åˆ†ç±»åˆ—ç¼ºå¤±å€¼"""
        cat_cols = df.select_dtypes(include='object').columns
        for col in cat_cols:
            if df[col].isnull().sum() > 0:
                if strategy == 'mode':
                    df[col].fillna(df[col].mode()[0], inplace=True)
                elif strategy == 'unknown':
                    df[col].fillna('Unknown', inplace=True)
        return df

    @staticmethod
    def drop_high_missing(df, threshold=0.5):
        """åˆ é™¤ç¼ºå¤±ç‡è¶…è¿‡é˜ˆå€¼çš„åˆ—"""
        missing_pct = df.isnull().sum() / len(df)
        cols_to_drop = missing_pct[missing_pct > threshold].index
        print(f"åˆ é™¤åˆ—: {list(cols_to_drop)}")
        return df.drop(columns=cols_to_drop)
```

### ç‰¹å¾å·¥ç¨‹

```python
class FeatureEngineer:
    """ç‰¹å¾å·¥ç¨‹å·¥å…·"""

    @staticmethod
    def create_datetime_features(df, date_col):
        """ä»æ—¥æœŸåˆ—æå–ç‰¹å¾"""
        df[date_col] = pd.to_datetime(df[date_col])
        df[f'{date_col}_year'] = df[date_col].dt.year
        df[f'{date_col}_month'] = df[date_col].dt.month
        df[f'{date_col}_day'] = df[date_col].dt.day
        df[f'{date_col}_dayofweek'] = df[date_col].dt.dayofweek
        df[f'{date_col}_is_weekend'] = df[date_col].dt.dayofweek >= 5
        return df

    @staticmethod
    def create_binned_features(df, col, bins, labels=None):
        """åˆ›å»ºåˆ†ç®±ç‰¹å¾"""
        df[f'{col}_binned'] = pd.cut(df[col], bins=bins, labels=labels)
        return df

    @staticmethod
    def encode_categorical(df, cols, method='onehot'):
        """ç¼–ç åˆ†ç±»å˜é‡"""
        if method == 'onehot':
            return pd.get_dummies(df, columns=cols, drop_first=True)
        elif method == 'label':
            from sklearn.preprocessing import LabelEncoder
            le = LabelEncoder()
            for col in cols:
                df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))
            return df
```

---

## æœºå™¨å­¦ä¹ å·¥ä½œæµ

### æ¨¡å‹è®­ç»ƒæ¨¡æ¿

```python
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression

class MLPipeline:
    """æœºå™¨å­¦ä¹ æµæ°´çº¿"""

    def __init__(self, X, y, test_size=0.2, random_state=42):
        self.X_train, self.X_test, self.y_train, self.y_test = \
            train_test_split(X, y, test_size=test_size, random_state=random_state)

        # æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)

        self.models = {}
        self.results = {}

    def train_models(self):
        """è®­ç»ƒå¤šä¸ªæ¨¡å‹"""
        models = {
            'Logistic Regression': LogisticRegression(max_iter=1000),
            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
            'Gradient Boosting': GradientBoostingClassifier(random_state=42)
        }

        for name, model in models.items():
            print(f"\nè®­ç»ƒ {name}...")

            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=5)

            # è®­ç»ƒ
            model.fit(self.X_train_scaled, self.y_train)

            # é¢„æµ‹
            y_pred = model.predict(self.X_test_scaled)

            # è¯„ä¼°
            self.models[name] = model
            self.results[name] = {
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'test_accuracy': accuracy_score(self.y_test, y_pred)
            }

            print(f"  CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
            print(f"  Test Accuracy: {accuracy_score(self.y_test, y_pred):.4f}")

    def compare_models(self):
        """æ¯”è¾ƒæ¨¡å‹ç»“æœ"""
        results_df = pd.DataFrame(self.results).T
        results_df = results_df.sort_values('test_accuracy', ascending=False)
        return results_df

    def get_best_model(self):
        """è·å–æœ€ä½³æ¨¡å‹"""
        best_name = max(self.results, key=lambda k: self.results[k]['test_accuracy'])
        return best_name, self.models[best_name]
```

### æ¨¡å‹è§£é‡Š

```python
def plot_feature_importance(model, feature_names, top_n=20):
    """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
    if hasattr(model, 'feature_importances_'):
        importance = model.feature_importances_
    elif hasattr(model, 'coef_'):
        importance = np.abs(model.coef_[0])
    else:
        print("æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§")
        return

    # æ’åº
    indices = np.argsort(importance)[::-1][:top_n]

    plt.figure(figsize=(10, 8))
    plt.barh(range(len(indices)), importance[indices])
    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
    plt.xlabel('é‡è¦æ€§')
    plt.title(f'Top {top_n} ç‰¹å¾é‡è¦æ€§')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()
```

---

## Notebook æœ€ä½³å®è·µ

### ä»£ç ç»„ç»‡

```
âœ… æ¨è
â–¡ ä½¿ç”¨ Markdown å•å…ƒæ ¼è§£é‡Šæ¯ä¸ªæ­¥éª¤
â–¡ æ¯ä¸ªå•å…ƒæ ¼åªåšä¸€ä»¶äº‹
â–¡ å°†é‡å¤ä»£ç æå–ä¸ºå‡½æ•°
â–¡ ä½¿ç”¨æœ‰æ„ä¹‰çš„å˜é‡å
â–¡ åœ¨ Notebook å¼€å¤´åˆ—å‡ºæ‰€æœ‰ä¾èµ–

âŒ é¿å…
â–¡ è¶…é•¿çš„ä»£ç å•å…ƒæ ¼
â–¡ æœªæ³¨é‡Šçš„å¤æ‚é€»è¾‘
â–¡ ç¡¬ç¼–ç çš„è·¯å¾„å’Œå‚æ•°
â–¡ æœªå¤„ç†çš„è­¦å‘Šä¿¡æ¯
â–¡ è¿è¡Œé¡ºåºä¾èµ–ï¼ˆéœ€ä»å¤´è¿è¡Œæ‰èƒ½å·¥ä½œï¼‰
```

### ç‰ˆæœ¬æ§åˆ¶å‹å¥½

```python
# åœ¨ Notebook å¼€å¤´æ·»åŠ 
%load_ext autoreload
%autoreload 2

# å°†æ ¸å¿ƒåŠŸèƒ½ç§»åˆ° .py æ–‡ä»¶
# ä¾‹å¦‚: src/preprocessing.py, src/models.py
from src.preprocessing import clean_data
from src.models import train_model
```

### ç”Ÿäº§åŒ–è½¬æ¢

```python
# å°† Notebook è½¬æ¢ä¸ºè„šæœ¬
# jupyter nbconvert --to script notebook.ipynb

# æˆ–ä½¿ç”¨ nbdev æ¡†æ¶
# pip install nbdev
# nbdev_export
```

---

## å¸¸ç”¨å¯è§†åŒ–æ¨¡æ¿

### åˆ†ç±»é—®é¢˜å¯è§†åŒ–

```python
def plot_classification_results(y_true, y_pred, labels=None):
    """åˆ†ç±»ç»“æœå¯è§†åŒ–"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])
    axes[0].set_title('æ··æ·†çŸ©é˜µ')
    axes[0].set_xlabel('é¢„æµ‹å€¼')
    axes[0].set_ylabel('çœŸå®å€¼')

    # åˆ†ç±»æŠ¥å‘Š
    report = classification_report(y_true, y_pred, output_dict=True)
    report_df = pd.DataFrame(report).T.iloc[:-3, :-1]
    report_df.plot(kind='bar', ax=axes[1])
    axes[1].set_title('åˆ†ç±»æŠ¥å‘Š')
    axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45)
    axes[1].legend(loc='lower right')

    plt.tight_layout()
    plt.show()
```

### å›å½’é—®é¢˜å¯è§†åŒ–

```python
def plot_regression_results(y_true, y_pred):
    """å›å½’ç»“æœå¯è§†åŒ–"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # é¢„æµ‹ vs çœŸå®
    axes[0].scatter(y_true, y_pred, alpha=0.5)
    axes[0].plot([y_true.min(), y_true.max()],
                 [y_true.min(), y_true.max()], 'r--', lw=2)
    axes[0].set_xlabel('çœŸå®å€¼')
    axes[0].set_ylabel('é¢„æµ‹å€¼')
    axes[0].set_title('é¢„æµ‹ vs çœŸå®')

    # æ®‹å·®åˆ†å¸ƒ
    residuals = y_true - y_pred
    axes[1].hist(residuals, bins=30, edgecolor='black', alpha=0.7)
    axes[1].axvline(x=0, color='r', linestyle='--')
    axes[1].set_xlabel('æ®‹å·®')
    axes[1].set_ylabel('é¢‘æ•°')
    axes[1].set_title('æ®‹å·®åˆ†å¸ƒ')

    plt.tight_layout()
    plt.show()
```

---

## ä¸ TAD æ¡†æ¶çš„é›†æˆ

åœ¨ TAD çš„æ•°æ®åˆ†ææµç¨‹ä¸­ï¼š

```
ä¸šåŠ¡é—®é¢˜ â†’ æ•°æ®è·å– â†’ EDA â†’ ç‰¹å¾å·¥ç¨‹ â†’ å»ºæ¨¡ â†’ è¯„ä¼° â†’ éƒ¨ç½²
               â†“
          [ æ­¤ Skill ]
```

### Gate Mapping

```yaml
Gate2_Design:
  ml_design:
    - Problem definition documented
    - Success metrics defined
    - Data sources identified
    - Experiment design outlined

Gate3_Testing:
  ml_validation:
    - Reproducibility verified
    - Cross-validation performed
    - Hold-out test evaluation
    - Baseline comparison

Gate4_Review:
  ml_documentation:
    - Model Card completed
    - Experiment tracked in MLflow/W&B
    - Code peer reviewed
    - Production readiness assessed
```

### Evidence Template

```markdown
## ML Evidence - [Model/Analysis Name]

**Date:** [Date]
**Developer:** [Name]
**MLflow Run ID:** [run_id]

---

### 1. Problem Definition

| Attribute | Value |
|-----------|-------|
| Business Objective | [Clear statement] |
| ML Task | Classification / Regression / Clustering |
| Success Metric | [e.g., F1 > 0.80] |
| Baseline | [Previous model or simple heuristic] |

### 2. Reproducibility Evidence

**Environment:**
\`\`\`
Python: 3.11.5
Key Packages:
  - scikit-learn==1.3.2
  - pandas==2.1.3
  - numpy==1.26.2
\`\`\`

**Random Seed:** 42

**Verification:**
| Run | Accuracy | F1 | Status |
|-----|----------|----| -------|
| Run 1 | 0.8721 | 0.8015 | âœ… |
| Run 2 | 0.8721 | 0.8015 | âœ… |
| Run 3 | 0.8721 | 0.8015 | âœ… |

### 3. Experiment Tracking

**MLflow Experiment:** customer-churn-v3
**Best Run:** rf_200trees_v3

| Metric | Value |
|--------|-------|
| Accuracy | 0.872 |
| Precision | 0.824 |
| Recall | 0.783 |
| F1 Score | 0.803 |
| AUC-ROC | 0.912 |

**Hyperparameters:**
\`\`\`json
{
  "n_estimators": 200,
  "max_depth": 12,
  "min_samples_split": 5,
  "class_weight": "balanced"
}
\`\`\`

### 4. Data Versioning

| Item | Version/Hash |
|------|--------------|
| Training Data | DVC: abc123 |
| Test Data | DVC: def456 |
| Feature Pipeline | v2.3 |

### 5. Model Card Status

- [x] Model details documented
- [x] Intended use specified
- [x] Training data described
- [x] Performance metrics by subgroup
- [x] Limitations documented
- [x] Ethical considerations reviewed

### 6. Review Sign-off

| Reviewer | Area | Status |
|----------|------|--------|
| [Name] | Code Quality | âœ… |
| [Name] | ML Methodology | âœ… |
| [Name] | Data Privacy | âœ… |

---

**ML Pipeline Ready:** âœ… Yes
**Model Registry:** models/churn-predictor-v2.1
```

### CI/CD for ML Projects

```yaml
# .github/workflows/ml-pipeline.yml
name: ML Pipeline

on:
  push:
    paths:
      - 'src/**'
      - 'data/**'
      - 'dvc.yaml'

jobs:
  test-reproducibility:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Pull DVC data
        run: dvc pull
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Run training (reproducibility check)
        run: |
          python train.py --seed 42
          python train.py --seed 42
          python scripts/compare_runs.py  # Verify identical results

      - name: Run tests
        run: pytest tests/ -v

      - name: Upload metrics
        uses: actions/upload-artifact@v4
        with:
          name: ml-metrics
          path: metrics.json
```

**ä½¿ç”¨åœºæ™¯**ï¼š
- æ¢ç´¢æ€§æ•°æ®åˆ†æ
- æœºå™¨å­¦ä¹ æ¨¡å‹å¼€å‘
- A/B æµ‹è¯•åˆ†æ
- é¢„æµ‹å»ºæ¨¡
- æ•°æ®æŠ¥å‘Šç”Ÿæˆ
- æ¨¡å‹å®¡è®¡ä¸åˆè§„

---

*æ­¤ Skill å¸®åŠ© Claude è¿›è¡Œé«˜æ•ˆã€å¯å¤ç°ã€è´Ÿè´£ä»»çš„æ•°æ®ç§‘å­¦å·¥ä½œæµã€‚*
