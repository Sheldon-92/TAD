# Pair Testing Brief

> This file is generated by Alex after Gate 4, for pair E2E testing with Claude Desktop Cowork.
> All sections are filled by Alex based on project context.
>
> **File location**: `.tad/pair-testing/TEST_BRIEF.md`
> **Screenshots**: `.tad/pair-testing/screenshots/`
> **Report output**: `.tad/pair-testing/PAIR_TEST_REPORT.md`

---

## 1. Product Overview

- **Product name**: {product_name}
- **One-line description**: {description}
- **Test environment URL**: {test_url}
- **Tech stack**: {tech_stack}

### Core User Scenarios
1. {scenario_1}
2. {scenario_2}
3. {scenario_3}

### Positioning Notes
{positioning_notes}

---

## 2. Test Scope

### Pages/Features to Test

| # | Page/Feature | Entry Path | Key Verification |
|---|-------------|------------|------------------|
| 1 | {page_1} | {path_1} | {focus_1} |
| 2 | {page_2} | {path_2} | {focus_2} |

### Out of Scope
- {exclusion_1}

---

## 3. Test Accounts & Data

- **Test account**: {test_account_info}
- **Test data location**: {test_data_location}
- **Test viewport**: {viewport_info}

---

## 4. Known Issues (Do Not Re-Report)

| Issue | Status |
|-------|--------|
| {known_issue_1} | {status_1} |

---

## 5. Focus Areas

### Design Decisions to Validate

| Decision | Chosen Approach | Alternative | Validation Goal |
|----------|----------------|-------------|-----------------|
| {decision_1} | {chosen_1} | {alternative_1} | {validation_goal_1} |

### UX Expectations (Cannot Be Verified by Code Review)
- {ux_expectation_1}
- {ux_expectation_2}

### E2E User Scenarios to Walk Through
1. {e2e_scenario_1}
2. {e2e_scenario_2}

### Interaction Details to Watch
- {interaction_detail_1}

---

## 6. Claude Desktop Pair Testing Collaboration Guide

> **Core principle**: This is a **conversational** pair test, not an autonomous test run.
> You **must stop** after each Round to collect human feedback before continuing.

### 6a. Role Definition

You are a **pair testing partner**, not an independent tester.

- **Your role**: Guide + Observer + Recorder + Questioner
- **Human's role**: Product expert + Operator + Final judge
- **Core principle**: Never finalize findings without human confirmation
- **Forbidden**: Do not make judgments for the human ("This is a Bug") — instead ask ("Does this behavior match your expectation?")
- **Forbidden**: Do not run multiple Rounds consecutively — you must STOP after each Round and wait for human feedback

### 6b. Overall Rhythm: Round-by-Round Conversational Testing

The entire test is divided into **{N} Rounds**, each is an independent test session.

**Key rule: One Round = One complete conversation cycle**

```
+--------------------------------------------------+
|  Round N Complete Lifecycle                       |
|                                                   |
|  1. Announce Round objective                      |
|  2. Guide human through each test step            |
|  3. Take screenshots at every test point          |
|  4. -- Round ends --                              |
|  5. Output Round summary (observations + shots)   |
|  6. STOP — Wait for human feedback               |
|  7. Human shares their views, feelings, issues    |
|  8. Merge human feedback into the record          |
|  9. Confirm: "Round N done. Ready for Round N+1?" |
|  10. Human confirms → then start next Round       |
|                                                   |
+--------------------------------------------------+
```

**Absolutely forbidden:**
- Running another Round without human confirmation
- Combining multiple Rounds in one output
- Skipping the screenshot step
- Skipping the human feedback collection step

### 6c. Pre-Test Preparation (Round 0)

Before starting any test:

1. **Read this entire brief**
2. **Confirm understanding with human**:
   > "I understand this test covers {scope}, focusing on {focus}. We'll go through {N} Rounds. After each Round I'll summarize and wait for your feedback. Test environment: {url}. Correct?"
3. **Confirm test environment is accessible**
4. **Confirm screenshot save location**: `.tad/pair-testing/screenshots/`
5. **Confirm any test data/accounts needed**

### 6d. Round Definitions

<!-- Alex fills this section with project-specific Rounds -->
<!-- Each Round should follow this template: -->

---

#### Round {N}: {Round Title} (Test Areas: {area_refs})

**Objective**: {what_this_round_validates}

**Test steps**:
1. {step_1}
2. {step_2}
3. {step_3}

**Screenshots**: At least {M} — {shot_1} + {shot_2}

**Round ends**: Output summary → STOP → Wait for human feedback

---

<!-- Repeat for each Round -->

### 6e. Screenshot Conventions

**Save location**: `.tad/pair-testing/screenshots/`

**Naming convention**:
```
.tad/pair-testing/screenshots/R{N}-{NN}-{description}.gif
```
- `R{N}`: Round number (R1, R2, ..., R8)
- `{NN}`: Sequence within Round (01, 02, ...)
- `{description}`: Short English description

**Example**:
```
.tad/pair-testing/screenshots/
  R1-01-onboarding-step1.gif
  R1-02-onboarding-complete.gif
  R2-01-photo-upload.gif
  R2-02-ocr-result.gif
  R3-01-settings-page.gif
```

**When to take screenshots**:
- **Every test step completed** — record normal state
- **When an issue is found** — record the issue (append `-issue` to filename)
- **When human requests** — anytime

**How to take screenshots (Claude in Chrome)**:

Use Chrome MCP's `gif_creator` tool. A single-frame GIF is essentially a static image.

**Steps**:
```
1. Start recording
   gif_creator(action: "start_recording", tabId: {tabId})

2. Perform a browser action (required — otherwise no frame is captured)
   - Use computer tool to click any element, or
   - Use navigate tool to refresh/navigate
   Note: A plain screenshot won't be recorded; you need an actual click/navigate action

3. Stop recording
   gif_creator(action: "stop_recording", tabId: {tabId})

4. Export and download
   gif_creator(action: "export", download: true, filename: "R1-01-description.gif", tabId: {tabId})
```

**Limitations & workaround**:
- Browser security: cannot specify download path — files go to the browser's default Downloads folder
- Workaround: after testing, human moves downloaded screenshots to `.tad/pair-testing/screenshots/`

**Example operation sequence**:
```
// Capture the Settings page
gif_creator(action: "start_recording", tabId: 123)
computer(action: "left_click", ref: "ref_22", tabId: 123)  // click to trigger frame capture
gif_creator(action: "stop_recording", tabId: 123)
gif_creator(action: "export", download: true, filename: "R3-01-settings-page.gif", tabId: 123)
// File downloads to Downloads folder; move to project dir after testing
```

**Fallback** (if gif_creator is unavailable):
- Tell human: "Please screenshot the current page, suggested name: `.tad/pair-testing/screenshots/R{N}-{NN}-{description}.png`"

### 6f. Standard Round Summary Format

After each Round, you **must** output this format, then **stop and wait for human feedback**:

```markdown
---
## Round {N} Summary: {Round Title}

### My Observations
- OK: {normal test point}
- OK: {normal test point}
- MAYBE: {possible issue — need your confirmation}
- ISSUE: {apparent anomaly — need your confirmation}

### Screenshots Taken
| File | Content |
|------|---------|
| R{N}-01-xxx.png | {description} |
| R{N}-02-xxx.png | {description} |

### Need Your Feedback
1. What do you think about the items marked MAYBE and ISSUE above?
2. Is there anything I missed but you noticed in this Round?
3. How does the overall experience feel?

STOP — Waiting for your feedback before continuing
---
```

**After receiving human feedback**:

```markdown
### Human Feedback (Round {N})
- {human's point 1}
- {human's point 2}
- {additional issues human noticed}

### Merged Findings (Round {N})
| # | Finding | Source | Severity | Status |
|---|---------|--------|----------|--------|
| R{N}-1 | {issue} | Me/Human/Both | P0/P1/P2 | confirmed/dismissed |

OK Round {N} done. Ready for Round {N+1}?
```

### 6g. Final Report Generation (After All Rounds Complete)

**Step 1: Consolidation**
- Merge all Round "Merged Findings" into one table
- Sort by severity: P0 → P1 → P2

**Step 2: Collect Human Overall Feedback**
> Ask the human:
> 1. "All {N} Rounds are done. How would you rate the overall experience (1-10)?"
> 2. "What are you most satisfied with?"
> 3. "What needs the most improvement?"
> 4. "Any missing features or unmet expectations?"
> 5. "Priority suggestions for next steps?"

**Step 3: Generate `.tad/pair-testing/PAIR_TEST_REPORT.md`**

Combine **your observations + human feedback** into a unified test report.
Save to `.tad/pair-testing/PAIR_TEST_REPORT.md`.

**Step 4: Human Final Review**
- Show the complete report to the human
- Save only after human confirms
- Remind human: "Report saved to `.tad/pair-testing/PAIR_TEST_REPORT.md`. Screenshots are in `.tad/pair-testing/screenshots/`. Next time you start /alex, Alex will auto-detect and process this report."

---

## 7. Output Requirements

### Screenshot Naming
```
.tad/pair-testing/screenshots/R{N}-{NN}-{page-name}.gif
```

### Report Format

Output file: `.tad/pair-testing/PAIR_TEST_REPORT.md`

The report should include:
- Test environment info (URL, browser, viewport)
- Per-Round test result summary
- Findings table (ID, description, severity P0/P1/P2, screenshot reference)
- Overall UX assessment
- Design improvement suggestions

---

## 8. Technical Notes

### Pre-Test Checklist

{technical_tips}

### Key File Reference

| Feature | Main File |
|---------|-----------|
| {feature_1} | {file_1} |
