# Pair Testing Brief

> This file is generated by Alex after Gate 4, for pair E2E testing.
> Supports two modes: **Claude Desktop Cowork** (MCP-based) or **Claude Code + Playwright** (CLI-based).
> All sections are filled by Alex based on project context.
>
> **File location**: `.tad/pair-testing/TEST_BRIEF.md`
> **Screenshots**: `.tad/pair-testing/screenshots/`
> **Report output**: `.tad/pair-testing/PAIR_TEST_REPORT.md`

---

## 1. Product Overview

- **Product name**: {product_name}
- **One-line description**: {description}
- **Test environment URL**: {test_url}
- **Tech stack**: {tech_stack}

### Core User Scenarios
1. {scenario_1}
2. {scenario_2}
3. {scenario_3}

### Positioning Notes
{positioning_notes}

---

## 2. Test Scope

### Pages/Features to Test

| # | Page/Feature | Entry Path | Key Verification |
|---|-------------|------------|------------------|
| 1 | {page_1} | {path_1} | {focus_1} |
| 2 | {page_2} | {path_2} | {focus_2} |

### Out of Scope
- {exclusion_1}

---

## 3. Test Accounts & Data

- **Test account**: {test_account_info}
- **Test data location**: {test_data_location}
- **Test viewport**: {viewport_info}

---

## 4. Known Issues (Do Not Re-Report)

| Issue | Status |
|-------|--------|
| {known_issue_1} | {status_1} |

---

## 5. Focus Areas

### Design Decisions to Validate

| Decision | Chosen Approach | Alternative | Validation Goal |
|----------|----------------|-------------|-----------------|
| {decision_1} | {chosen_1} | {alternative_1} | {validation_goal_1} |

### UX Expectations (Cannot Be Verified by Code Review)
- {ux_expectation_1}
- {ux_expectation_2}

### E2E User Scenarios to Walk Through
1. {e2e_scenario_1}
2. {e2e_scenario_2}

### Interaction Details to Watch
- {interaction_detail_1}

---

## 6. Pair Testing Collaboration Guide

> **Core principle**: This is a **conversational** pair test, not an autonomous test run.
> You **must stop** after each step/action to collect human feedback before continuing.
> Two modes are supported — choose one based on your environment.

### 6.0 Mode Selection

| Mode | Environment | Browser Control | Screenshots |
|------|-------------|-----------------|-------------|
| **Mode A: Claude Desktop** | Claude Desktop + Chrome MCP | MCP `gif_creator` + `computer` tools | .gif via MCP export |
| **Mode B: Claude Code + Playwright** | Claude Code CLI + Playwright | Command-driven browser controller | .png via Playwright `page.screenshot()` |

**How to choose**:
- If you have Chrome MCP tools (`gif_creator`, `computer`, `navigate`) → Mode A
- If you are in Claude Code CLI → Mode B

---

### 6a. Role Definition

You are a **pair testing partner**, not an independent tester.

- **Your role**: Guide + Observer + Recorder + Questioner
- **Human's role**: Product expert + Operator + Final judge
- **Core principle**: Never finalize findings without human confirmation
- **Forbidden**: Do not make judgments for the human ("This is a Bug") — instead ask ("Does this behavior match your expectation?")
- **Forbidden**: Do not run multiple steps or Rounds consecutively — you must STOP and wait for human feedback

### 6b. Overall Rhythm: Step-by-Step Conversational Testing

The test is divided into **{N} Rounds**, each covering a page/feature area.
Within each Round, you proceed **one action at a time**.

**Key rule: One Action = One conversation turn**

```
+--------------------------------------------------+
|  Single Action Lifecycle                          |
|                                                   |
|  1. Announce what you will do next                |
|  2. Execute ONE action (click, navigate, etc.)    |
|  3. Take screenshot immediately                   |
|  4. Show screenshot to human (or describe state)  |
|  5. STOP — Ask human for feedback                |
|  6. Human responds (feedback, or "continue")      |
|  7. Record any findings                           |
|  8. Proceed to next action                        |
|                                                   |
+--------------------------------------------------+
```

**Round lifecycle wraps multiple actions**:

```
+--------------------------------------------------+
|  Round N Complete Lifecycle                       |
|                                                   |
|  1. Announce Round objective                      |
|  2. Execute actions ONE BY ONE (see above)        |
|  3. After all actions in Round complete:           |
|     → Output Round summary                        |
|     → STOP — Wait for human overall feedback     |
|  4. Human shares views, feelings, issues          |
|  5. Merge human feedback into the record          |
|  6. Confirm: "Round N done. Ready for Round N+1?" |
|  7. Human confirms → then start next Round        |
|                                                   |
+--------------------------------------------------+
```

**Absolutely forbidden:**
- Executing multiple actions without stopping for feedback between them
- Running another Round without human confirmation
- Combining multiple Rounds in one output
- Skipping the screenshot step
- Skipping the human feedback collection step
- Auto-closing the browser or setting timeouts that interrupt the session

### 6c. Pre-Test Preparation (Round 0)

Before starting any test:

1. **Read this entire brief**
2. **Confirm understanding with human**:
   > "I understand this test covers {scope}, focusing on {focus}. We'll go through {N} Rounds. After each action I'll screenshot and wait for your feedback. Test environment: {url}. Correct?"
3. **Confirm test environment is accessible**
4. **Confirm screenshot save location**: `.tad/pair-testing/screenshots/`
5. **Confirm any test data/accounts needed**
6. **Launch browser** (see Mode-specific setup below)

### 6d. Round Definitions

<!-- Alex fills this section with project-specific Rounds -->
<!-- Each Round should follow this template: -->

---

#### Round {N}: {Round Title} (Test Areas: {area_refs})

**Objective**: {what_this_round_validates}

**Test steps** (execute one at a time, screenshot + feedback after each):
1. {step_1}
2. {step_2}
3. {step_3}

**Screenshots**: At least {M} — {shot_1} + {shot_2}

**Round ends**: Output Round summary → STOP → Wait for human feedback

---

<!-- Repeat for each Round -->

### 6e. Screenshot Conventions

**Save location**: `.tad/pair-testing/screenshots/`

**Naming convention**:
```
.tad/pair-testing/screenshots/R{N}-{NN}-{description}.png
```
- `R{N}`: Round number (R1, R2, ..., R8)
- `{NN}`: Sequence within Round (01, 02, ...)
- `{description}`: Short English description
- Extension: `.png` (Mode B) or `.gif` (Mode A)

**Example**:
```
.tad/pair-testing/screenshots/
  R1-01-homepage-top.png
  R1-02-homepage-scrolled.png
  R2-01-upload-page.png
  R2-02-file-selected.png
  R3-01-coaching-greeting.png
```

**When to take screenshots**:
- **After every single action** — record the resulting state
- **When an issue is found** — append `-issue` to filename
- **When human says "I did something"** — immediately screenshot to see what changed
- **When human requests** — anytime
- **On page transitions** — capture both before and after

### 6e-A. Mode A: Screenshots via Claude Desktop (Chrome MCP)

Use Chrome MCP's `gif_creator` tool:

```
1. gif_creator(action: "start_recording", tabId: {tabId})
2. Perform a browser action (click/navigate — required for frame capture)
3. gif_creator(action: "stop_recording", tabId: {tabId})
4. gif_creator(action: "export", download: true, filename: "R1-01-description.gif", tabId: {tabId})
```

**Limitation**: Files download to browser's default folder. Human moves them to `.tad/pair-testing/screenshots/` after testing.

### 6e-B. Mode B: Screenshots via Claude Code (Playwright)

Use the command-driven browser controller (see Section 6h for setup):

```bash
# Take a screenshot
echo '{"action":"screenshot","name":"R1-01-homepage-top.png"}' > /tmp/pw-cmd.json
# Wait for result
for i in $(seq 1 15); do [ -f /tmp/pw-result.json ] && cat /tmp/pw-result.json && rm /tmp/pw-result.json && break; sleep 1; done
```

Screenshots save directly to `.tad/pair-testing/screenshots/` — no manual file moving needed.

**Full-page screenshot**:
```bash
echo '{"action":"screenshot","name":"R1-04-fullpage.png","fullPage":true}' > /tmp/pw-cmd.json
```

### 6f. Standard Round Summary Format

After each Round, you **must** output this format, then **stop and wait for human feedback**:

```markdown
---
## Round {N} Summary: {Round Title}

### My Observations
- OK: {normal test point}
- OK: {normal test point}
- MAYBE: {possible issue — need your confirmation}
- ISSUE: {apparent anomaly — need your confirmation}

### Screenshots Taken
| File | Content |
|------|---------|
| R{N}-01-xxx.png | {description} |
| R{N}-02-xxx.png | {description} |

### Need Your Feedback
1. What do you think about the items marked MAYBE and ISSUE above?
2. Is there anything I missed but you noticed in this Round?
3. How does the overall experience feel?

STOP — Waiting for your feedback before continuing
---
```

**After receiving human feedback**:

```markdown
### Human Feedback (Round {N})
- {human's point 1}
- {human's point 2}
- {additional issues human noticed}

### Merged Findings (Round {N})
| # | Finding | Source | Severity | Status |
|---|---------|--------|----------|--------|
| R{N}-1 | {issue} | Me/Human/Both | P0/P1/P2 | confirmed/dismissed |

OK Round {N} done. Ready for Round {N+1}?
```

### 6g. Final Report Generation (After All Rounds Complete)

**Step 1: Consolidation**
- Merge all Round "Merged Findings" into one table
- Sort by severity: P0 → P1 → P2

**Step 2: Collect Human Overall Feedback**
> Ask the human:
> 1. "All {N} Rounds are done. How would you rate the overall experience (1-10)?"
> 2. "What are you most satisfied with?"
> 3. "What needs the most improvement?"
> 4. "Any missing features or unmet expectations?"
> 5. "Priority suggestions for next steps?"

**Step 3: Generate `.tad/pair-testing/PAIR_TEST_REPORT.md`**

Combine **your observations + human feedback** into a unified test report.
Save to `.tad/pair-testing/PAIR_TEST_REPORT.md`.

**Step 4: Human Final Review**
- Show the complete report to the human
- Save only after human confirms
- Remind human: "Report saved to `.tad/pair-testing/PAIR_TEST_REPORT.md`. Screenshots are in `.tad/pair-testing/screenshots/`. Next time you start /alex, Alex will auto-detect and process this report."

### 6h. Mode B Setup: Command-Driven Browser Controller (Claude Code + Playwright)

> This section only applies to **Mode B** (Claude Code CLI).
> Mode A users (Claude Desktop) skip this section entirely.

#### Why a Command-Driven Controller?

Claude Code cannot maintain a persistent browser connection across conversation turns.
The solution: a **long-running Node.js process** that keeps the browser open and accepts commands via file-based RPC.

#### Architecture

```
┌─────────────┐     write JSON      ┌──────────────────┐     Playwright     ┌─────────┐
│ Claude Code │ ──────────────────→ │ Browser Controller│ ──────────────── → │ Browser │
│   (CLI)     │ ← read result JSON  │  (Node.js bg)    │ ← page state       │ (headed)│
└─────────────┘                      └──────────────────┘                     └─────────┘
     │                                       │
     │  /tmp/pw-cmd.json (command)           │  Polls every 1 second
     │  /tmp/pw-result.json (result)         │  Auto-screenshots after each action
```

#### Setup Steps

**Step 1: Ensure Playwright is installed in the project**
```bash
cd {project_dir}
npm ls playwright  # check if installed
# If not: npm install --save-dev playwright
```

**Step 2: Create the controller script**

Create `pair-test-browser.js` in the project root:

```javascript
const { chromium } = require('playwright');
const fs = require('fs');

const CMD_FILE = '/tmp/pw-cmd.json';
const RESULT_FILE = '/tmp/pw-result.json';
const SCREENSHOT_DIR = '{project_dir}/.tad/pair-testing/screenshots';

(async () => {
  const browser = await chromium.launch({ headless: false });
  const context = await browser.newContext({ viewport: { width: 1280, height: 720 } });
  const page = await context.newPage();

  if (fs.existsSync(CMD_FILE)) fs.unlinkSync(CMD_FILE);
  if (fs.existsSync(RESULT_FILE)) fs.unlinkSync(RESULT_FILE);

  console.log('BROWSER_READY');

  while (true) {
    await new Promise(r => setTimeout(r, 1000));
    if (!fs.existsSync(CMD_FILE)) continue;

    let cmd;
    try {
      const raw = fs.readFileSync(CMD_FILE, 'utf-8');
      cmd = JSON.parse(raw);
      fs.unlinkSync(CMD_FILE);
    } catch (e) { continue; }

    const result = { ok: true, action: cmd.action };

    try {
      switch (cmd.action) {
        case 'goto':
          await page.goto(cmd.url, { waitUntil: 'networkidle', timeout: 120000 });
          result.url = page.url();
          break;
        case 'click':
          await page.locator(cmd.selector).first().click();
          await page.waitForTimeout(cmd.wait || 2000);
          result.url = page.url();
          break;
        case 'click_text':
          await page.locator('a, button, span, label').filter({ hasText: new RegExp(cmd.text, 'i') }).first().click();
          await page.waitForTimeout(cmd.wait || 2000);
          result.url = page.url();
          break;
        case 'upload':
          await page.locator('input[type="file"]').setInputFiles(cmd.path);
          await page.waitForTimeout(cmd.wait || 2000);
          break;
        case 'type':
          await page.locator(cmd.selector).fill(cmd.text);
          await page.waitForTimeout(500);
          break;
        case 'scroll':
          await page.evaluate((y) => window.scrollBy(0, y), cmd.y || 500);
          await page.waitForTimeout(800);
          break;
        case 'screenshot':
          await page.screenshot({ path: SCREENSHOT_DIR + '/' + cmd.name, fullPage: cmd.fullPage || false });
          result.screenshot = cmd.name;
          break;
        case 'state':
          result.url = page.url();
          result.title = await page.title();
          const h = await page.locator('h1, h2').first().textContent().catch(() => 'N/A');
          result.heading = h;
          const btns = await page.locator('button').allTextContents();
          result.buttons = btns.filter(b => b.trim()).map(b => b.trim());
          const inputs = await page.locator('input[type="file"]').count();
          result.hasFileInput = inputs > 0;
          break;
        case 'wait':
          await page.waitForTimeout(cmd.ms || 5000);
          break;
        case 'wait_url':
          await page.waitForURL(cmd.pattern, { timeout: cmd.timeout || 120000 });
          result.url = page.url();
          break;
        case 'eval':
          result.value = await page.evaluate(cmd.code);
          break;
        case 'quit':
          await browser.close();
          process.exit(0);
          break;
        default:
          result.ok = false;
          result.error = 'Unknown action: ' + cmd.action;
      }
    } catch (e) {
      result.ok = false;
      result.error = e.message;
    }

    if (cmd.action !== 'screenshot' && cmd.action !== 'state' && cmd.action !== 'quit') {
      const autoName = 'auto-' + Date.now() + '.png';
      await page.screenshot({ path: SCREENSHOT_DIR + '/' + autoName, fullPage: false }).catch(() => {});
      result.autoScreenshot = autoName;
    }

    result.currentUrl = page.url();
    fs.writeFileSync(RESULT_FILE, JSON.stringify(result, null, 2));
    console.log('CMD: ' + cmd.action + ' -> ' + (result.ok ? 'OK' : 'ERR: ' + result.error));
  }
})();
```

**Step 3: Launch the controller (background)**
```bash
node pair-test-browser.js &
# Or via Claude Code: run_in_background: true
```

**Step 4: Ensure screenshots directory exists**
```bash
mkdir -p .tad/pair-testing/screenshots
```

#### Command Reference

| Action | JSON | Description |
|--------|------|-------------|
| Navigate | `{"action":"goto","url":"http://localhost:3000"}` | Go to URL |
| Click by selector | `{"action":"click","selector":"#submit-btn"}` | Click element |
| Click by text | `{"action":"click_text","text":"Start"}` | Click element containing text |
| Upload file | `{"action":"upload","path":"/path/to/file.pdf"}` | Upload via file input |
| Type text | `{"action":"type","selector":"#email","text":"test@example.com"}` | Fill input field |
| Scroll | `{"action":"scroll","y":500}` | Scroll down (negative = up) |
| Screenshot | `{"action":"screenshot","name":"R1-01-desc.png"}` | Capture viewport |
| Full screenshot | `{"action":"screenshot","name":"R1-01-full.png","fullPage":true}` | Capture full page |
| Page state | `{"action":"state"}` | Get URL, title, heading, buttons |
| Wait | `{"action":"wait","ms":5000}` | Wait N milliseconds |
| Evaluate JS | `{"action":"eval","code":"document.title"}` | Run JS in page |
| Quit | `{"action":"quit"}` | Close browser and exit |

#### Command Execution Pattern

```bash
# 1. Send command
echo '{"action":"screenshot","name":"R1-01-homepage.png"}' > /tmp/pw-cmd.json

# 2. Wait for result (poll up to 15 seconds)
for i in $(seq 1 15); do [ -f /tmp/pw-result.json ] && cat /tmp/pw-result.json && rm /tmp/pw-result.json && break; sleep 1; done
```

#### Critical Rules for Mode B

1. **Always use headed mode** (`headless: false`) — the human must see the browser
2. **No timeouts** — the browser controller runs indefinitely until `quit` command
3. **One action per turn** — execute one command, screenshot, show human, wait for feedback
4. **Monitor human actions** — when human says they clicked/typed something, immediately take a screenshot to see the result
5. **Auto-screenshots** — the controller auto-captures after every non-screenshot action; use these as backup evidence
6. **Read screenshots** — after taking a screenshot, use `Read` tool to view the .png file and describe what you see to the human
7. **Clean up** — send `{"action":"quit"}` when testing is complete; delete `pair-test-browser.js` from project root

---

## 7. Output Requirements

### Screenshot Naming
```
.tad/pair-testing/screenshots/R{N}-{NN}-{page-name}.png
```

### Report Format

Output file: `.tad/pair-testing/PAIR_TEST_REPORT.md`

The report should include:
- Test environment info (URL, browser, viewport)
- Per-Round test result summary
- Findings table (ID, description, severity P0/P1/P2, screenshot reference)
- Overall UX assessment
- Design improvement suggestions

---

## 8. Technical Notes

### Pre-Test Checklist

{technical_tips}

### Key File Reference

| Feature | Main File |
|---------|-----------|
| {feature_1} | {file_1} |
